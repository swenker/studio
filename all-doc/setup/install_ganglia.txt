ganglia installation
server:
one gweb
one gmetad [gweb -->gmetad ->gmond ->hsflow(gmond)] [gmetad gets data from gmond by data source configuration]
one gmond.

client:
gmond on client can be replaced by hsflowd, hsflow will send data to remote gmond via udp 6343. 
[centos]one hsflowd[not use gmond any more] 
[windows]
So the remote gmond.conf should disable multi cast and enable bind to it's ip. because most of the network will disable broadcast.


通过自动yum安装的版本太低，不支持sflow，所以需要从源码安装，安装后配置文件用的路径是在/usr/local/etc下。
而源码包含所有

配置方面：
如果gmetad需要从多个cluster收集数据，那么需要在 gmetad.conf中加入
data_source "cpp cluster" localhost
data_source "hadoop cluster" nn-qasvc1.hadoop.mscc.cn
data_source "rcms cluster" rmadmin.stsvc.mscc.cn

每个数据源都是一个gmond(一个gmond可能收集了多个hsflowd提交的数据)，所以在data_source指向的gmond节点的配置文件gmond.conf中，无需指明host，因为是gmetad来获取。
gweb从gmetad获取数据。
如果用gmond收集多个client的数据，则需要更改gmond.conf  delete gmond metrics and add the following:

注意，这里用的是udp协议，对应server端的iptables要开放相应权限。
/* Channel to receive sFlow datagrams */
udp_recv_channel {
  port = 6343
}

/* Optional sFlow settings */
sflow {
 udp_port = 6343
 accept_vm_metrics = yes
 accept_jvm_metrics = yes
 multiple_jvm_instances = no
 accept_http_metrics = yes
 multiple_http_instances = no
 accept_memcache_metrics = yes
 multiple_memcache_instances = no
}
----------------------------------------------------------------------------------


常见的问题：
gmond 没有收到hsflowd的node状态---没有其他host列表 ,一般都是防火墙引起的，如果添加了防火墙规则没效果，try stop it
常见之一就是没有添加udp的规则。
-A INPUT -i eth0 -p tcp -m tcp --dport 8649 -j ACCEPT
-A INPUT -i eth0 -p tcp -m tcp --dport 6343 -j ACCEPT
-A INPUT -i eth0 -p udp -m udp --dport 6343 -j ACCEPT

hsflowd
http://blog.sflow.com/2010/10/installing-host-sflow-on-linux-server.html
should use unicast

if gmond is installed from source,the config path is in :/usr/local/etc , it does not exist if it is installed from source.
  so gmond -t >gmond.conf will generate it.
if installed from yum,  /etc/ganglia/gmond.conf
/usr/local/sbin/gmond

1.install && basics:
download the package and install,some extra dependent should be installed.
apr apr-devel lconfuse libconfuse-devel expat expat-devel pcre pcre-devel zlib zlib-devel rrdtool rrdtool-devel

./configure will complains this.
if configure is ok,you will get the feedback .

configure --with-gmetad
[gmond] --data collection and it will report data to gmetad. 
some of the networks forbid broadcast,so unicast can be used.
it use multicase by default.

if gmond serves as a monitor client:
open /etc/ganglia/gmond.conf and add host=your central collector and comment out other m-cast line.
gmond -d 10 

========================
[gmetad] (the Ganglia Meta Daemon) is the service that collects metric data from other
gmetad and gmond sources and stores their state to disk in RRD format.
#sudo yum install ganglia-gmetad

ganglia-gmetad will be always installed on the same server with web.

ganglia web 
sudo yum -y install ganglia-gmond ganglia-gmetad  ganglia-web
--------------------------------------------------------
there is readme in the folder.
edit Makefile 
cp apache.conf /etc/httpd/conf.d/ganglia.conf
make install

service restart httpd


cluster: consists of hosts[cluster peers have the save multicast address] 
grid: consists of cluster

try default config and then telnet localhost 8649,if ok. then try it again.

if a host is dead and always on the list, then service gmetad restart will resolve this. 

http://sourceforge.net/apps/trac/ganglia/wiki/ganglia_quick_start
check gmetad
telnet 127.0.0.1 8652
telnet 127.0.0.1 8649

/cluster-name

deaf node
mute node

vist web console:
There was an error collecting ganglia data (127.0.0.1:8652): fsockopen error: Permission denied

getenforce :enforcing
setenforce 0 
refresh page.

07.Graphite:
  a tool to display data.
  gmetad can send data to this.

09.with hadoop:

*.sink.ganglia.class=org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31
*.sink.ganglia.period=10

namenode.sink.ganglia.servers=nn-qasvc1.hadoop.mscc.cn:8649
datanode.sink.ganglia.servers=nn-qasvc1.hadoop.mscc.cn:8649
jobtracker.sink.ganglia.servers=nn-qasvc1.hadoop.mscc.cn:8649
tasktracker.sink.ganglia.servers=nn-qasvc1.hadoop.mscc.cn:8649
maptask.sink.ganglia.servers=nn-qasvc1.hadoop.mscc.cn:8649
reducetask.sink.ganglia.servers=nn-qasvc1.hadoop.mscc.cn:8649

sbin/stop-dfs.sh --config /etc/hadoop/conf


cloud env:
unicast:
config
/etc/ganglia 
conf.php has same path.
$gmetad_root = "/var/lib/ganglia";
$rrds = "$gmetad_root/rrds";
installed: /usr/share/ganglia



ganglia web 3.6 has tools to fetch fdata from nagios?

#######steps on how to configure a new server to be monitored:
1.yum install -y ganglia-gmond
2.iptables:
  -A INPUT -i eth0 -p tcp -m tcp --dport 8649 -j ACCEPT
  and restart

3.vim /etc/ganglia/gmond.conf
  add host point to your collector

4.service gmond start

Done.

########Windows support
http://host-sflow.sourceforge.net


https://github.com/ganglia

ganglia install:
download the core file and unzip it. check README.
it contains gmond and gmetad and gexec?
gmond -t ?gmond.conf
gmetad is not installed by default:
http://ganglia.info/
http://sourceforge.net/projects/ganglia/files/ganglia%20monitoring%20core/
default=`/usr/local/etc/gmetad.conf')
https://github.com/ganglia

thereis a setuid =nobody ,this should the same the owner of /var/lib/ganglia/rrds

RRDs directory '/var/lib/ganglia/rrds' is not readable.
mkdir -p /var/lib/ganglia/rrds


web 3.6 just copy file but can not be opened?
1.ganglia-web config:
  conf_default.php
  we can put conf.php with customized values to override the default ones.
  change dwoo as 775

--------------------
[root@lab01 etc]# cat /etc/httpd/conf.d/ganglia.conf
  #
  # Ganglia monitoring system php web frontend
  #
  
  #Alias /ganglia /usr/share/ganglia
  Alias /ganglia /var/lib/ganglia-web

  <Location /ganglia>
    Order deny,allow
    Deny from all
    Allow from 106.120.65.62
    Allow from ::1
    # Allow from .example.com
  </Location>
----------------------

###############slfow
integrete with slfow: just install on windows and then let gmond collect it.
?6343?

how to add a new node to ganglia metrics:
http://sourceforge.net/projects/host-sflow/files/Latest/
wget rmp file :http://downloads.sourceforge.net/project/host-sflow/Latest/hsflowd-1.24.1-1.x86_64.rpm?r=http%3A%2F%2Fsourceforge.net%2Fprojects%2Fhost-sflow%2Ffiles%2FLatest%2F&ts=1404960758&use_mirror=jaist

scp hsflowd-1.24.1-1.x86_64.rpm db01:~/ && ssh db01 'sudo rpm -ivh /home/pssh/hsflowd-1.24.1-1.x86_64.rpm'
scp hsflowd.conf db01:~/   && ssh db01 "sudo /bin/cp /home/pssh/hsflowd.conf /etc/   && sudo service hsflowd restart"

pscp -e error -h /data/deploy-farm/hosts/cpp-push.svc hsflowd-1.24.1-1.x86_64.rpm /home/pssh/
pscp -e error -h /data/deploy-farm/hosts/cpp-push.svc hsflowd.conf /home/pssh/
pssh -e error -h /data/deploy-farm/hosts/cpp-push.svc -P "sudo rpm -ivh /home/pssh/hsflowd-1.24.1-1.x86_64.rpm && sudo /bin/cp /home/pssh/hsflowd.conf /etc/   && sudo service hsflowd restart"
there is no need to update iptables.


# vim /etc/hsflowd.conf  report to lab01.qasvc
# hsflowd -d 

pssh -e error -h /data/deploy-farm/hosts/hadoop.qasvc -P "sudo service hsflowd restart"

The Host sFlow agent is an open source implementation of the sFlow standard for server monitoring. 
http://host-sflow.sourceforge.net/documentation.php

######Host sFlow instead of gmond to monitor servers
Metrics:The standard set of sFlow metrics includes the core Ganglia metrics as well as additional disk I/O, swap, interrupt and virtual machine statistics. 

example:
/usr/bin/gmetric --name Current_Users --value `who |wc -l` --type int32 --unit
current_users

Running the command periodically using crontab allows Ganglia to track the metric. 

host sflow ,sub agent

host sflow can get the client name and show it to ganglia

if ganglia frong UI show ip instead of hostname while the client agent is hsflowd,just stop gmond for a while,then start it and  restart gmetad.

##########
http://www.inmon.com/technology/sflowVersion5.php

tomcat --sflowvalve.jar should be put to tomcat/lib
<Valve className="com.sflow.catalina.SFlowValve" />
and after restart tomcat, it will take some time to get it from the web page
named httpd metric.


JVM :java -javaagent:$CATALINA_BASE/sflowagent.jar 
this will show up immediately after restart jvm.

java -javaagent:sflowagent.jar\
     -Dsflow.hostname=vm.test\
     -Dsflow.uuid=564dd23c-453d-acd2-fa64-85da86713620\
     -Dsflow.dsindex=80001\
     MyApp

Arguments can be passed to the sFlow module as system properties:

sflow.hostname: optionally assign a "hostname" to identify the virtual machine. Choose a naming strategy that is helpful in identifying virtual machine instances, for example: "hadoop.node1" etc. The hostname is exported in the sFlow host_descr structure. 
sflow.uuid: optionally assign a UUID to the virtual machine so that it can be uniquely identified. The UUID is exported in the sFlow host_descr structure.
sflow.dsindex: uniquely identifies the data source associated with this virtual machine on this server. The dsindex number only needs to be set if more than one virtual machine is running on the server. For virtual machines offering network services, use TCP/UDP port number associated with the service as the dsindex, otherwise, use numbers in the range 65536-99999 to avoid clashes with other sFlow agents on the server.


